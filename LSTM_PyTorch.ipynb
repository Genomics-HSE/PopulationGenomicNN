{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM-PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_epW_x_1CC4R",
        "colab_type": "text"
      },
      "source": [
        "Firstly, add $__init__.py$ from utilities dir (to do it tap < in the left)  \n",
        "\n",
        "Secondly, rename $__init.py__$ to $ utilities.py $ \n",
        "\n",
        "To install msprime we need to get some dev tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAP30xOqAt0x",
        "colab_type": "code",
        "outputId": "e65b9acc-af78-4c18-f5a3-573f33be37c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!apt-get install python-dev libgsl0-dev\n",
        "!python3 -m pip install msprime"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Note, selecting 'libgsl-dev' instead of 'libgsl0-dev'\n",
            "python-dev is already the newest version (2.7.15~rc1-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libgsl23 libgslcblas0\n",
            "Suggested packages:\n",
            "  gsl-ref-psdoc | gsl-doc-pdf | gsl-doc-info | gsl-ref-html\n",
            "The following NEW packages will be installed:\n",
            "  libgsl-dev libgsl23 libgslcblas0\n",
            "0 upgraded, 3 newly installed, 0 to remove and 7 not upgraded.\n",
            "Need to get 1,926 kB of archives.\n",
            "After this operation, 9,474 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgslcblas0 amd64 2.4+dfsg-6 [79.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgsl23 amd64 2.4+dfsg-6 [823 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgsl-dev amd64 2.4+dfsg-6 [1,023 kB]\n",
            "Fetched 1,926 kB in 3s (642 kB/s)\n",
            "Selecting previously unselected package libgslcblas0:amd64.\n",
            "(Reading database ... 145674 files and directories currently installed.)\n",
            "Preparing to unpack .../libgslcblas0_2.4+dfsg-6_amd64.deb ...\n",
            "Unpacking libgslcblas0:amd64 (2.4+dfsg-6) ...\n",
            "Selecting previously unselected package libgsl23:amd64.\n",
            "Preparing to unpack .../libgsl23_2.4+dfsg-6_amd64.deb ...\n",
            "Unpacking libgsl23:amd64 (2.4+dfsg-6) ...\n",
            "Selecting previously unselected package libgsl-dev.\n",
            "Preparing to unpack .../libgsl-dev_2.4+dfsg-6_amd64.deb ...\n",
            "Unpacking libgsl-dev (2.4+dfsg-6) ...\n",
            "Setting up libgslcblas0:amd64 (2.4+dfsg-6) ...\n",
            "Setting up libgsl23:amd64 (2.4+dfsg-6) ...\n",
            "Setting up libgsl-dev (2.4+dfsg-6) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Collecting msprime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/35/a4d83d89f26f4d1c886d9924128280e039dcd1d569c0d30074b674ab7c02/msprime-0.7.4.tar.gz (460kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from msprime) (1.17.5)\n",
            "Collecting tskit\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/45/9a702226e6e4ba929c705d0845a4492acfbfaf95deea37599b4cfb7a300d/tskit-0.2.3.tar.gz (208kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 37.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from tskit->msprime) (2.8.0)\n",
            "Collecting svgwrite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/2e/f36cfec1da6162055b884e6366074cff18475a9538559ceae0c0bc58e186/svgwrite-1.3.1-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from tskit->msprime) (2.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->tskit->msprime) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from svgwrite->tskit->msprime) (2.4.6)\n",
            "Building wheels for collected packages: msprime, tskit\n",
            "  Building wheel for msprime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for msprime: filename=msprime-0.7.4-cp36-cp36m-linux_x86_64.whl size=440975 sha256=2822a9806ea63a82587f9aaaa47ea356bbfde90da2444353c5053e161a704b60\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/2e/87/ee21d00be6a0a077584160307b98f4cd91f38908fc3cdf8108\n",
            "  Building wheel for tskit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tskit: filename=tskit-0.2.3-cp36-cp36m-linux_x86_64.whl size=493409 sha256=fb52d4b87ca1e34e136a013673b4239c242f361f92c4b9b5040a4f8ad0dcd902\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/29/df/bc5fcbfd879ef87e6931e42da82f7ad5c471e5ffc02e8f8299\n",
            "Successfully built msprime tskit\n",
            "Installing collected packages: svgwrite, tskit, msprime\n",
            "Successfully installed msprime-0.7.4 svgwrite-1.3.1 tskit-0.2.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "09pNtLrD-Dkd",
        "colab": {}
      },
      "source": [
        "import utilities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwIhx0PlCMWx",
        "colab_type": "text"
      },
      "source": [
        "Create data generator "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTZAQrrn8dZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "events = utilities.generate_demographic_events()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBvBHyUQA_nh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Ne = 1\n",
        "rho =  1.6*10e-9*Ne\n",
        "mu  = 1.25*10e-8*Ne\n",
        "num_repl = int(1e5) # 5\n",
        "l = int(3e3)\n",
        "\n",
        "dg = utilities.DataGenerator(recombination_rate=rho,\n",
        "                             mutation_rate=mu,\n",
        "                             demographic_events=events,\n",
        "                             num_replicates=num_repl, lengt=l)\n",
        "data = dg.run_simulation()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7WVo1paKgnP",
        "colab_type": "text"
      },
      "source": [
        "Create datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBGNCW4jLXjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "number_train_examples = int(num_repl*.9)\n",
        "\n",
        "trX, trY = [], []\n",
        "for i in range(number_train_examples):\n",
        "  example = next(dg)\n",
        "  trX.append(example[0])\n",
        "  trY.append(example[1])\n",
        "\n",
        "teX, teY = [], []\n",
        "for example in dg:\n",
        "  teX.append(example[0])\n",
        "  teY.append(example[1])\n",
        "\n",
        "\n",
        "del dg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bfjz6v5WLIZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        super(MyDataset, self).__init__()\n",
        "        assert x.shape[0] == y.shape[0] # assuming shape[0] = dataset size\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.y.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index], self.y[index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6lrxAT3XAks",
        "colab_type": "code",
        "outputId": "9177401f-e726-4181-cce1-0a69e6c6bb85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "# device = 'cpu'\n",
        "\n",
        "input = torch.from_numpy(np.array(trX, dtype=np.float_)) #.to(device)\n",
        "target = torch.from_numpy(np.array(trY)) # .to(device)\n",
        "test_input = torch.from_numpy(np.array(teX, dtype=np.float_))# .to(device)\n",
        "test_target = torch.from_numpy(np.array(teY))# .to(device)\n",
        "\n",
        "del trX, trY, teX, teY\n",
        "\n",
        "traindata = MyDataset(input, target)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXAi7NAKCgK3",
        "colab_type": "text"
      },
      "source": [
        "Specify NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0armriyLkeu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "N = 51\n",
        "\n",
        "class Sequence(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Sequence, self).__init__()\n",
        "        self.lstm1 = nn.LSTMCell(1, 51)\n",
        "        self.lstm2 = nn.LSTMCell(51, 51)\n",
        "        self.linear = nn.Linear(51, 1)\n",
        "\n",
        "    def forward(self, input, future = 0):\n",
        "        outputs = []\n",
        "        h_t = torch.zeros(input.size(0), 51, dtype=torch.double).to(device)\n",
        "        c_t = torch.zeros(input.size(0), 51, dtype=torch.double).to(device)\n",
        "        h_t2 = torch.zeros(input.size(0), 51, dtype=torch.double).to(device)\n",
        "        c_t2 = torch.zeros(input.size(0), 51, dtype=torch.double).to(device)\n",
        "\n",
        "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
        "            h_t, c_t = self.lstm1(input_t, (h_t, c_t))\n",
        "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
        "            output = self.linear(h_t2)\n",
        "            outputs += [output]\n",
        "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
        "        return outputs\n",
        "\n",
        "def make_train_step(model, loss_fn, optimizer):\n",
        "    # Builds function that performs a step in the train loop\n",
        "    def train_step(x, y):\n",
        "        # Sets model to TRAIN mode\n",
        "        model.train()\n",
        "        # Makes predictions\n",
        "        yhat = model(x)\n",
        "        # Computes loss\n",
        "        loss = loss_fn(y, yhat)\n",
        "        # Computes gradients\n",
        "        loss.backward()\n",
        "        # Updates parameters and zeroes gradients\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        # Returns the loss\n",
        "        return loss.item()\n",
        "    \n",
        "    # Returns the function that will be called inside the train loop\n",
        "    return train_step\n",
        "\n",
        "loss_per_step = []\n",
        "def main():\n",
        "    # set random seed to 0\n",
        "    np.random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    # load data and make training set\n",
        "    train_loader = torch.utils.data.DataLoader(traindata, batch_size=64, \n",
        "                                              shuffle=True)\n",
        "\n",
        "    # build the model\n",
        "    seq = Sequence()\n",
        "    seq = seq.to(device)\n",
        "    seq.double()\n",
        "    criterion = nn.MSELoss()\n",
        "    criterion = criterion.to(device)\n",
        "    # \n",
        "    optimizer = optim.SGD(seq.parameters(), lr=.001)\n",
        "    #begin to train\n",
        "    \n",
        "    train_step = make_train_step(seq, criterion, optimizer)\n",
        "    total_step = len(train_loader)\n",
        "    \n",
        "    for i in range(15):\n",
        "        print('STEP: ', i)\n",
        "        total_loss = 0\n",
        "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
        "          loss = train_step(x_batch.to(device), y_batch.to(device))\n",
        "          total_loss += loss\n",
        "          if i % 5 == 0: print(f'BATCH: {i+1}/{total_step} LOSS: {loss}')\n",
        "        loss_per_step.append(total_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vKfc0CKZQMY",
        "colab_type": "code",
        "outputId": "a6dc60fe-9d2e-44a3-b3c7-04de3a1a2231",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# %%script false\n",
        "main()\n",
        "loss_per_step"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "STEP:  0\n",
            "BATCH: 1/1407 LOSS: 5191889.266543173\n",
            "BATCH: 6/1407 LOSS: 4645469.583396519\n",
            "BATCH: 11/1407 LOSS: 5293222.802241087\n",
            "BATCH: 16/1407 LOSS: 4065448.650027007\n",
            "BATCH: 21/1407 LOSS: 2107837.633462674\n",
            "BATCH: 26/1407 LOSS: 3427755.312646055\n",
            "BATCH: 31/1407 LOSS: 1493515.819444495\n",
            "BATCH: 36/1407 LOSS: 2148274.099311213\n",
            "BATCH: 41/1407 LOSS: 1276548.5994584775\n",
            "BATCH: 46/1407 LOSS: 2958069.2189081684\n",
            "BATCH: 51/1407 LOSS: 3960645.8394038845\n",
            "BATCH: 56/1407 LOSS: 2638955.72074782\n",
            "BATCH: 61/1407 LOSS: 2497273.5393991075\n",
            "BATCH: 66/1407 LOSS: 3682395.924624732\n",
            "BATCH: 71/1407 LOSS: 2253107.2372786095\n",
            "BATCH: 76/1407 LOSS: 1740091.69749368\n",
            "BATCH: 81/1407 LOSS: 2135142.4117426467\n",
            "BATCH: 86/1407 LOSS: 2372261.8022592436\n",
            "BATCH: 91/1407 LOSS: 1756205.0499965185\n",
            "BATCH: 96/1407 LOSS: 2216065.623397198\n",
            "BATCH: 101/1407 LOSS: 2165881.144110333\n",
            "BATCH: 106/1407 LOSS: 1612537.7478806279\n",
            "BATCH: 111/1407 LOSS: 2445657.257538476\n",
            "BATCH: 116/1407 LOSS: 2557552.0849812333\n",
            "BATCH: 121/1407 LOSS: 2957930.963734914\n",
            "BATCH: 126/1407 LOSS: 2531926.7147846394\n",
            "BATCH: 131/1407 LOSS: 1629568.0778832538\n",
            "BATCH: 136/1407 LOSS: 3311450.7463091947\n",
            "BATCH: 141/1407 LOSS: 1459060.1116365988\n",
            "BATCH: 146/1407 LOSS: 1977709.6963617676\n",
            "BATCH: 151/1407 LOSS: 2487723.32168474\n",
            "BATCH: 156/1407 LOSS: 1779700.9599783903\n",
            "BATCH: 161/1407 LOSS: 1862431.7023434138\n",
            "BATCH: 166/1407 LOSS: 1254908.3178877288\n",
            "BATCH: 171/1407 LOSS: 3519816.071091379\n",
            "BATCH: 176/1407 LOSS: 1578871.0361666465\n",
            "BATCH: 181/1407 LOSS: 2829815.4001488737\n",
            "BATCH: 186/1407 LOSS: 3383495.2798647247\n",
            "BATCH: 191/1407 LOSS: 1975331.061187143\n",
            "BATCH: 196/1407 LOSS: 1705024.5865951085\n",
            "BATCH: 201/1407 LOSS: 2241462.429779553\n",
            "BATCH: 206/1407 LOSS: 2322601.618731163\n",
            "BATCH: 211/1407 LOSS: 2455826.434556163\n",
            "BATCH: 216/1407 LOSS: 2478947.9111144478\n",
            "BATCH: 221/1407 LOSS: 2609785.7518065213\n",
            "BATCH: 226/1407 LOSS: 1580531.66554398\n",
            "BATCH: 231/1407 LOSS: 1723761.1637872967\n",
            "BATCH: 236/1407 LOSS: 1797393.9200606996\n",
            "BATCH: 241/1407 LOSS: 3083985.419834108\n",
            "BATCH: 246/1407 LOSS: 2653595.7503346526\n",
            "BATCH: 251/1407 LOSS: 3001175.6840882096\n",
            "BATCH: 256/1407 LOSS: 2775634.965385242\n",
            "BATCH: 261/1407 LOSS: 2587991.076800163\n",
            "BATCH: 266/1407 LOSS: 1587840.4073738346\n",
            "BATCH: 271/1407 LOSS: 2664520.655824462\n",
            "BATCH: 276/1407 LOSS: 3625577.780986725\n",
            "BATCH: 281/1407 LOSS: 1535518.3107465652\n",
            "BATCH: 286/1407 LOSS: 2061151.259380796\n",
            "BATCH: 291/1407 LOSS: 2738231.1138272244\n",
            "BATCH: 296/1407 LOSS: 4134982.1328969686\n",
            "BATCH: 301/1407 LOSS: 1489497.7909803512\n",
            "BATCH: 306/1407 LOSS: 2441919.7804014888\n",
            "BATCH: 311/1407 LOSS: 1728973.7546442929\n",
            "BATCH: 316/1407 LOSS: 2126467.584209\n",
            "BATCH: 321/1407 LOSS: 2837109.232298478\n",
            "BATCH: 326/1407 LOSS: 1413677.2579874995\n",
            "BATCH: 331/1407 LOSS: 1899665.2585930473\n",
            "BATCH: 336/1407 LOSS: 3308713.2672154643\n",
            "BATCH: 341/1407 LOSS: 3123506.7597825495\n",
            "BATCH: 346/1407 LOSS: 1683412.0332989397\n",
            "BATCH: 351/1407 LOSS: 2276398.338587522\n",
            "BATCH: 356/1407 LOSS: 2285681.6750706583\n",
            "BATCH: 361/1407 LOSS: 2199536.9797077235\n",
            "BATCH: 366/1407 LOSS: 1832502.5443211829\n",
            "BATCH: 371/1407 LOSS: 2412157.6442578146\n",
            "BATCH: 376/1407 LOSS: 2410725.471299443\n",
            "BATCH: 381/1407 LOSS: 3643472.943800719\n",
            "BATCH: 386/1407 LOSS: 1960720.6496438463\n",
            "BATCH: 391/1407 LOSS: 1715590.6287629707\n",
            "BATCH: 396/1407 LOSS: 1549376.4143329656\n",
            "BATCH: 401/1407 LOSS: 2423807.6093704854\n",
            "BATCH: 406/1407 LOSS: 2616824.1729662437\n",
            "BATCH: 411/1407 LOSS: 2427994.6313875476\n",
            "BATCH: 416/1407 LOSS: 3102706.4508001427\n",
            "BATCH: 421/1407 LOSS: 4369688.455574463\n",
            "BATCH: 426/1407 LOSS: 1831945.3214591416\n",
            "BATCH: 431/1407 LOSS: 3642350.339113532\n",
            "BATCH: 436/1407 LOSS: 2513437.2897946895\n",
            "BATCH: 441/1407 LOSS: 1598630.5068989256\n",
            "BATCH: 446/1407 LOSS: 1845966.796648674\n",
            "BATCH: 451/1407 LOSS: 1691345.8905659663\n",
            "BATCH: 456/1407 LOSS: 2546140.0251326435\n",
            "BATCH: 461/1407 LOSS: 3032685.3289010064\n",
            "BATCH: 466/1407 LOSS: 1896216.813842197\n",
            "BATCH: 471/1407 LOSS: 3447014.750155346\n",
            "BATCH: 476/1407 LOSS: 3038107.9746266175\n",
            "BATCH: 481/1407 LOSS: 1713562.4251297226\n",
            "BATCH: 486/1407 LOSS: 2792820.0154698114\n",
            "BATCH: 491/1407 LOSS: 1381870.6888753155\n",
            "BATCH: 496/1407 LOSS: 1606548.8695660117\n",
            "BATCH: 501/1407 LOSS: 1768615.8830261845\n",
            "BATCH: 506/1407 LOSS: 2282167.2185957963\n",
            "BATCH: 511/1407 LOSS: 2165172.7801850615\n",
            "BATCH: 516/1407 LOSS: 2363371.0993778603\n",
            "BATCH: 521/1407 LOSS: 2786771.9940164774\n",
            "BATCH: 526/1407 LOSS: 2334684.5483961846\n",
            "BATCH: 531/1407 LOSS: 6786555.3288676925\n",
            "BATCH: 536/1407 LOSS: 2259275.763863986\n",
            "BATCH: 541/1407 LOSS: 3784274.622025151\n",
            "BATCH: 546/1407 LOSS: 2910072.5502822897\n",
            "BATCH: 551/1407 LOSS: 4068931.5507890945\n",
            "BATCH: 556/1407 LOSS: 2224899.919828242\n",
            "BATCH: 561/1407 LOSS: 2104862.669823067\n",
            "BATCH: 566/1407 LOSS: 2251035.9099097876\n",
            "BATCH: 571/1407 LOSS: 2662110.0016548554\n",
            "BATCH: 576/1407 LOSS: 4235286.636789552\n",
            "BATCH: 581/1407 LOSS: 3751213.3289767876\n",
            "BATCH: 586/1407 LOSS: 2151802.8520124955\n",
            "BATCH: 591/1407 LOSS: 1560378.6592419932\n",
            "BATCH: 596/1407 LOSS: 2737745.6515708542\n",
            "BATCH: 601/1407 LOSS: 1200996.358046581\n",
            "BATCH: 606/1407 LOSS: 2523282.5614518384\n",
            "BATCH: 611/1407 LOSS: 2612062.8541285503\n",
            "BATCH: 616/1407 LOSS: 2150099.3672021776\n",
            "BATCH: 621/1407 LOSS: 1564356.0612650285\n",
            "BATCH: 626/1407 LOSS: 2976122.534969073\n",
            "BATCH: 631/1407 LOSS: 1453873.52104569\n",
            "BATCH: 636/1407 LOSS: 4143325.405522983\n",
            "BATCH: 641/1407 LOSS: 3135298.1544469683\n",
            "BATCH: 646/1407 LOSS: 1913789.7116534477\n",
            "BATCH: 651/1407 LOSS: 2395365.8919491093\n",
            "BATCH: 656/1407 LOSS: 1673558.4562306774\n",
            "BATCH: 661/1407 LOSS: 2835935.2998539624\n",
            "BATCH: 666/1407 LOSS: 2527544.8531352063\n",
            "BATCH: 671/1407 LOSS: 2929596.5491651427\n",
            "BATCH: 676/1407 LOSS: 5047903.341236046\n",
            "BATCH: 681/1407 LOSS: 2201490.0670369426\n",
            "BATCH: 686/1407 LOSS: 2259514.125303382\n",
            "BATCH: 691/1407 LOSS: 3920747.873359223\n",
            "BATCH: 696/1407 LOSS: 4070969.2085128827\n",
            "BATCH: 701/1407 LOSS: 2206353.593825477\n",
            "BATCH: 706/1407 LOSS: 1585731.7674010152\n",
            "BATCH: 711/1407 LOSS: 2401209.474727893\n",
            "BATCH: 716/1407 LOSS: 1974696.148137208\n",
            "BATCH: 721/1407 LOSS: 1536142.0181638198\n",
            "BATCH: 726/1407 LOSS: 2356307.701630682\n",
            "BATCH: 731/1407 LOSS: 2414231.7978064097\n",
            "BATCH: 736/1407 LOSS: 3337339.6163023626\n",
            "BATCH: 741/1407 LOSS: 4162732.4491974274\n",
            "BATCH: 746/1407 LOSS: 2024249.788866587\n",
            "BATCH: 751/1407 LOSS: 2091450.662986864\n",
            "BATCH: 756/1407 LOSS: 1827744.248573939\n",
            "BATCH: 761/1407 LOSS: 2356579.3688647025\n",
            "BATCH: 766/1407 LOSS: 3268170.03469366\n",
            "BATCH: 771/1407 LOSS: 1700342.4588829908\n",
            "BATCH: 776/1407 LOSS: 4059847.151584983\n",
            "BATCH: 781/1407 LOSS: 1839717.8792595873\n",
            "BATCH: 786/1407 LOSS: 2663954.1268520043\n",
            "BATCH: 791/1407 LOSS: 1948169.9884824462\n",
            "BATCH: 796/1407 LOSS: 3831420.0295658326\n",
            "BATCH: 801/1407 LOSS: 2825704.861962371\n",
            "BATCH: 806/1407 LOSS: 2874883.9293617033\n",
            "BATCH: 811/1407 LOSS: 1991910.9706949994\n",
            "BATCH: 816/1407 LOSS: 3523281.108953856\n",
            "BATCH: 821/1407 LOSS: 1669657.6889524034\n",
            "BATCH: 826/1407 LOSS: 3329188.593185067\n",
            "BATCH: 831/1407 LOSS: 1888846.8167829672\n",
            "BATCH: 836/1407 LOSS: 1338417.5888517736\n",
            "BATCH: 841/1407 LOSS: 2588112.6031830977\n",
            "BATCH: 846/1407 LOSS: 3774908.3511707666\n",
            "BATCH: 851/1407 LOSS: 2235533.431401705\n",
            "BATCH: 856/1407 LOSS: 2651544.252433249\n",
            "BATCH: 861/1407 LOSS: 2456728.8020371576\n",
            "BATCH: 866/1407 LOSS: 4009557.452813476\n",
            "BATCH: 871/1407 LOSS: 3320433.599761394\n",
            "BATCH: 876/1407 LOSS: 2732081.9350239118\n",
            "BATCH: 881/1407 LOSS: 4026712.837103963\n",
            "BATCH: 886/1407 LOSS: 2666585.4970274945\n",
            "BATCH: 891/1407 LOSS: 3564551.9495090176\n",
            "BATCH: 896/1407 LOSS: 2545062.6406473373\n",
            "BATCH: 901/1407 LOSS: 2346225.825438068\n",
            "BATCH: 906/1407 LOSS: 2781842.079264532\n",
            "BATCH: 911/1407 LOSS: 2194705.997630626\n",
            "BATCH: 916/1407 LOSS: 1436128.141105264\n",
            "BATCH: 921/1407 LOSS: 3944532.2131876023\n",
            "BATCH: 926/1407 LOSS: 2371007.347925197\n",
            "BATCH: 931/1407 LOSS: 2781156.660340823\n",
            "BATCH: 936/1407 LOSS: 2343362.107394213\n",
            "BATCH: 941/1407 LOSS: 2849308.432036155\n",
            "BATCH: 946/1407 LOSS: 2666367.1775419964\n",
            "BATCH: 951/1407 LOSS: 1677883.5826413229\n",
            "BATCH: 956/1407 LOSS: 2320212.5779629736\n",
            "BATCH: 961/1407 LOSS: 2732533.9571205494\n",
            "BATCH: 966/1407 LOSS: 2322852.7922801077\n",
            "BATCH: 971/1407 LOSS: 2195051.1834702385\n",
            "BATCH: 976/1407 LOSS: 4223589.708678583\n",
            "BATCH: 981/1407 LOSS: 3496694.943798738\n",
            "BATCH: 986/1407 LOSS: 1182990.8249265677\n",
            "BATCH: 991/1407 LOSS: 3346584.797452345\n",
            "BATCH: 996/1407 LOSS: 2052775.937217482\n",
            "BATCH: 1001/1407 LOSS: 2685262.937142039\n",
            "BATCH: 1006/1407 LOSS: 3551454.2085832087\n",
            "BATCH: 1011/1407 LOSS: 2385458.3876216067\n",
            "BATCH: 1016/1407 LOSS: 2192348.7001514533\n",
            "BATCH: 1021/1407 LOSS: 2569098.1532265646\n",
            "BATCH: 1026/1407 LOSS: 1855173.9100585363\n",
            "BATCH: 1031/1407 LOSS: 3425447.5587621056\n",
            "BATCH: 1036/1407 LOSS: 3256663.288128857\n",
            "BATCH: 1041/1407 LOSS: 2574792.718695987\n",
            "BATCH: 1046/1407 LOSS: 2108982.1529197586\n",
            "BATCH: 1051/1407 LOSS: 3044443.242579745\n",
            "BATCH: 1056/1407 LOSS: 2144287.8337288634\n",
            "BATCH: 1061/1407 LOSS: 2806493.1923789997\n",
            "BATCH: 1066/1407 LOSS: 2744565.0372890504\n",
            "BATCH: 1071/1407 LOSS: 1766282.0381003383\n",
            "BATCH: 1076/1407 LOSS: 2156584.136904831\n",
            "BATCH: 1081/1407 LOSS: 3641873.341656293\n",
            "BATCH: 1086/1407 LOSS: 2761612.7236065897\n",
            "BATCH: 1091/1407 LOSS: 1502348.4591076851\n",
            "BATCH: 1096/1407 LOSS: 2022567.6623922233\n",
            "BATCH: 1101/1407 LOSS: 1601770.9332568122\n",
            "BATCH: 1106/1407 LOSS: 2584561.9579631463\n",
            "BATCH: 1111/1407 LOSS: 2575415.234561437\n",
            "BATCH: 1116/1407 LOSS: 3453949.3358300123\n",
            "BATCH: 1121/1407 LOSS: 5510960.776100879\n",
            "BATCH: 1126/1407 LOSS: 4355174.6213532705\n",
            "BATCH: 1131/1407 LOSS: 2901459.423162975\n",
            "BATCH: 1136/1407 LOSS: 1438604.8862793753\n",
            "BATCH: 1141/1407 LOSS: 2607608.2556242947\n",
            "BATCH: 1146/1407 LOSS: 2466287.943560187\n",
            "BATCH: 1151/1407 LOSS: 2067331.8495946366\n",
            "BATCH: 1156/1407 LOSS: 1997823.2167885595\n",
            "BATCH: 1161/1407 LOSS: 3059757.4981028424\n",
            "BATCH: 1166/1407 LOSS: 2203346.189403665\n",
            "BATCH: 1171/1407 LOSS: 3623989.68728227\n",
            "BATCH: 1176/1407 LOSS: 1467631.0492580435\n",
            "BATCH: 1181/1407 LOSS: 1751136.3026791455\n",
            "BATCH: 1186/1407 LOSS: 1506915.8701571638\n",
            "BATCH: 1191/1407 LOSS: 3270226.628258281\n",
            "BATCH: 1196/1407 LOSS: 2018710.8957055158\n",
            "BATCH: 1201/1407 LOSS: 4074627.0514366636\n",
            "BATCH: 1206/1407 LOSS: 2635773.496518174\n",
            "BATCH: 1211/1407 LOSS: 1439627.3256018506\n",
            "BATCH: 1216/1407 LOSS: 3906273.8757143985\n",
            "BATCH: 1221/1407 LOSS: 1368774.7256356436\n",
            "BATCH: 1226/1407 LOSS: 2408587.0531094354\n",
            "BATCH: 1231/1407 LOSS: 3178234.092258655\n",
            "BATCH: 1236/1407 LOSS: 2641339.8847911283\n",
            "BATCH: 1241/1407 LOSS: 1813426.1578647168\n",
            "BATCH: 1246/1407 LOSS: 4152115.133644993\n",
            "BATCH: 1251/1407 LOSS: 2438288.963731797\n",
            "BATCH: 1256/1407 LOSS: 1604762.4613781173\n",
            "BATCH: 1261/1407 LOSS: 2516047.499223964\n",
            "BATCH: 1266/1407 LOSS: 5733294.056442269\n",
            "BATCH: 1271/1407 LOSS: 3843787.6962653324\n",
            "BATCH: 1276/1407 LOSS: 1849242.5336203526\n",
            "BATCH: 1281/1407 LOSS: 2285675.4650597586\n",
            "BATCH: 1286/1407 LOSS: 1552524.398575942\n",
            "BATCH: 1291/1407 LOSS: 3390690.6227986105\n",
            "BATCH: 1296/1407 LOSS: 2228806.684510995\n",
            "BATCH: 1301/1407 LOSS: 3488342.2996520484\n",
            "BATCH: 1306/1407 LOSS: 1938122.2479110023\n",
            "BATCH: 1311/1407 LOSS: 2379093.131475809\n",
            "BATCH: 1316/1407 LOSS: 3554549.6316758394\n",
            "BATCH: 1321/1407 LOSS: 1912663.754610278\n",
            "BATCH: 1326/1407 LOSS: 1981270.088773304\n",
            "BATCH: 1331/1407 LOSS: 2308699.080988683\n",
            "BATCH: 1336/1407 LOSS: 1692499.6948016353\n",
            "BATCH: 1341/1407 LOSS: 2526656.1966385725\n",
            "BATCH: 1346/1407 LOSS: 980854.7563513574\n",
            "BATCH: 1351/1407 LOSS: 2532084.238426047\n",
            "BATCH: 1356/1407 LOSS: 2156185.53131474\n",
            "BATCH: 1361/1407 LOSS: 2576661.655445538\n",
            "BATCH: 1366/1407 LOSS: 4569310.481277099\n",
            "BATCH: 1371/1407 LOSS: 1139521.9925294307\n",
            "BATCH: 1376/1407 LOSS: 4083426.6970061576\n",
            "BATCH: 1381/1407 LOSS: 3137514.048494064\n",
            "BATCH: 1386/1407 LOSS: 2365501.3409735593\n",
            "BATCH: 1391/1407 LOSS: 2103440.208514948\n",
            "BATCH: 1396/1407 LOSS: 2860845.8916806104\n",
            "BATCH: 1401/1407 LOSS: 2450700.4478990147\n",
            "BATCH: 1406/1407 LOSS: 1397842.7677456245\n",
            "STEP:  1\n",
            "BATCH: 1/1407 LOSS: 2671859.198742142\n",
            "BATCH: 6/1407 LOSS: 2497035.4821709376\n",
            "BATCH: 11/1407 LOSS: 2501548.772794251\n",
            "BATCH: 16/1407 LOSS: 1456198.380563202\n",
            "BATCH: 21/1407 LOSS: 2485010.1655992595\n",
            "BATCH: 26/1407 LOSS: 2580824.679252695\n",
            "BATCH: 31/1407 LOSS: 2027590.115067582\n",
            "BATCH: 36/1407 LOSS: 4159304.7203378216\n",
            "BATCH: 41/1407 LOSS: 4000708.2545841993\n",
            "BATCH: 46/1407 LOSS: 2659366.6871656803\n",
            "BATCH: 51/1407 LOSS: 2465799.8892913586\n",
            "BATCH: 56/1407 LOSS: 2105608.3924969654\n",
            "BATCH: 61/1407 LOSS: 2380808.9230907164\n",
            "BATCH: 66/1407 LOSS: 1545202.4398281372\n",
            "BATCH: 71/1407 LOSS: 3487797.9078531926\n",
            "BATCH: 76/1407 LOSS: 2211771.7909447867\n",
            "BATCH: 81/1407 LOSS: 2056581.243686717\n",
            "BATCH: 86/1407 LOSS: 1979131.2719173662\n",
            "BATCH: 91/1407 LOSS: 2688608.8821339984\n",
            "BATCH: 96/1407 LOSS: 1992558.9598370786\n",
            "BATCH: 101/1407 LOSS: 1841951.2754718654\n",
            "BATCH: 106/1407 LOSS: 2401368.558663567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LAB2rxUZQI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from https://github.com/pytorch/examples/blob/master/time_sequence_prediction/train.py\n",
        "\n",
        "# It's work, but only for little dataset - we need to specify mini-batches \n",
        "# to make it work;\n",
        "\n",
        "%%script false\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "N = 51\n",
        "\n",
        "class Sequence(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Sequence, self).__init__()\n",
        "        self.lstm1 = nn.LSTMCell(1, 51)\n",
        "        self.lstm2 = nn.LSTMCell(51, 51)\n",
        "        self.linear = nn.Linear(51, 1)\n",
        "\n",
        "    def forward(self, input, future = 0):\n",
        "        outputs = []\n",
        "        h_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
        "        c_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
        "        h_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
        "        c_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
        "\n",
        "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
        "            h_t, c_t = self.lstm1(input_t, (h_t, c_t))\n",
        "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
        "            output = self.linear(h_t2)\n",
        "            outputs += [output]\n",
        "        for i in range(future):# if we should predict the future\n",
        "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
        "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
        "            output = self.linear(h_t2)\n",
        "            outputs += [output]\n",
        "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
        "        return outputs\n",
        "\n",
        "def main():\n",
        "    # set random seed to 0\n",
        "    np.random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    # load data and make training set\n",
        "\n",
        "    #data = torch.load('traindata.pt')\n",
        "    input = torch.from_numpy(np.array(trX, dtype=np.float_))\n",
        "    target = torch.from_numpy(np.array(trY))\n",
        "    test_input = torch.from_numpy(np.array(teX, dtype=np.float_))\n",
        "    test_target = torch.from_numpy(np.array(teY))\n",
        "    \n",
        "    # build the model\n",
        "    seq = Sequence()\n",
        "    seq.double()\n",
        "    criterion = nn.MSELoss()\n",
        "    # use LBFGS as optimizer since we can load the whole data to train\n",
        "    optimizer = optim.LBFGS(seq.parameters(), lr=0.8)\n",
        "    #begin to train\n",
        "    for i in range(15):\n",
        "        print('STEP: ', i)\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            out = seq(input)\n",
        "            loss = criterion(out, target)\n",
        "            print('loss:', loss.item())\n",
        "            loss.backward()\n",
        "            return loss\n",
        "        optimizer.step(closure)\n",
        "        # begin to predict, no need to track gradient here\n",
        "        with torch.no_grad():\n",
        "            future = 1000\n",
        "            pred = seq(test_input, future=future)\n",
        "            loss = criterion(pred[:, :-future], test_target)\n",
        "            print('test loss:', loss.item())\n",
        "            y = pred.detach().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjIoYjkmZQU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6GKKG-OZQXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN8m3DEmZQZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLh-8z6AZQcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcT2HKWrZQfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC8r0HWcOBb4",
        "colab_type": "text"
      },
      "source": [
        "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "\n",
        "https://towardsdatascience.com/lstm-for-time-series-prediction-de8aeb26f2ca\n",
        "\n",
        "http://chandlerzuo.github.io/blog/2017/11/darnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdTCpYkS5C1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1Za1YU25C4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VMsofxH5C6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkzWN3Cw5C9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYZr3V8B1uXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}